{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with Decision Trees\n",
    "\n",
    "This notebook will show how to use a decision tree model to classify banknotes as either authentic or inauthentic. \n",
    "\n",
    "## What is a Decision Tree?\n",
    "A decision tree is a flowchart-like structure where an internal node represents a feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. First, it learns to partition based on the attribute value. Then, it partitions the tree in a recursive manner called recursive partitioning. This flowchart-like structure helps you in decision-making. It's visualization like a flowchart diagram that easily mimics human-level thinking. That is why decision trees are easy to understand and interpret.\n",
    "\n",
    "![Decision Trees Digram](./res/dtree.jpg 'Decision Tree')\n",
    "\n",
    "\n",
    "In this notebook, we start by importing the necessary library modules and functions as follows:\n",
    "\n",
    "* The `pyplot` module from the `matplotlib` library for creating the visulizations needed.\n",
    "* We import the `pandas` library for loading the data from a CSV file.\n",
    "* From the `sklearn.model_selection`, we import the `train_test_split()` function which we use to split our data into training and testing data.\n",
    "* From the `sklearn`, we import the `tree` module which we use when plotting the decsion tree digram.\n",
    "* From the `sklearn.tree`, we import the `DecisionTreeClassifier()` class which we use as a classification model in this notebook.\n",
    "* Also, from the `sklearn.tree`, we import the `export_text()` function which allow us to build a text report showing the rules of a decision tree.\n",
    "* From the `sklearn.metrics`, we import the `f1_score()`, `accuracy_score()`, `precision_score()` and `recall_score()` functions to evaluate the our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-10-12T12:32:21.622595Z",
     "iopub.status.busy": "2021-10-12T12:32:21.621723Z",
     "iopub.status.idle": "2021-10-12T12:32:21.639781Z",
     "shell.execute_reply": "2021-10-12T12:32:21.639226Z",
     "shell.execute_reply.started": "2021-10-12T12:32:21.622545Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # the visualizations  framework\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import train_test_split # split the data in train/test\n",
    "from sklearn import tree   # used for plotting the tree\n",
    "from sklearn.tree import DecisionTreeClassifier # our classifier\n",
    "from sklearn.tree import export_text # export the decsion as a text report\n",
    "from sklearn.metrics import f1_score, accuracy_score, \\\n",
    "                            precision_score, recall_score   # evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data\n",
    "\n",
    "The data set we use here is stored in the CSV file under `\"./data/bank_notes.csv\"`. We use the `pandas` library short name (i.e., `pd`) to call the `read_csv` method, which takes the file path as an argument. The method returns a data frame that is assigned to the variable `data`. We use that variable to show the top five rows (using the `head` method) and then print the summary statistics (count, mean, standard deviation, etc.) using the `describe` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "data = pd.read_csv(\"./data/bank_notes.csv\")\n",
    "\n",
    "print(\"First five rows:\")\n",
    "print(data.head())  # dsiplay the first 5 rows\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"Summary statsitics:\")\n",
    "print(data.describe())  # show the the summary statistics for the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [BankNote Authentication Dataset](https://archive.ics.uci.edu/ml/datasets/banknote+authentication) is about distinguishing genuine and forged banknotes. Data were extracted from images that were taken from genuine and forged banknote-like specimens. For digitization, an industrial camera usually used for print inspection was used. The final images have 400x 400 pixels. Due to the object lens and distance to the investigated object gray-scale pictures with a resolution of about 660 dpi were gained. A Wavelet Transform tool was used to extract features from these images.\n",
    "\n",
    "### Attribute Information\n",
    "\n",
    "* **variance** of Wavelet Transformed image (continuous).\n",
    "* **skewness** of Wavelet Transformed image (continuous).\n",
    "* **curtosis** of Wavelet Transformed image (continuous).\n",
    "* **entropy** of image (continuous).\n",
    "* **class** The target label (discrete): $0$ for *authentic* and $1$ for *inauthentic*.\n",
    "\n",
    "The number of examples in the data set (as shown in the summary statistics) is 1372. \n",
    "\n",
    "Read [James D. McCaffrey's blog bost](https://bit.ly/3pc6eQt) to learn about the label of this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting the Data into Train/Test\n",
    "\n",
    "Before splitting the data, we get two slices from the data frame (i.e., `data`). The first slice contains the features, which we assign to a variable named `X`. The second slice contains the target (i.e., `price`), which we set to the `y` variable. \n",
    "\n",
    "We then use the `train_test_split()` to split our data into training and testing data randomly. The function expects two sequences of data: \n",
    "\n",
    "* `X` sequence containing the features.\n",
    "* `y` sequence containing the target label.\n",
    "\n",
    "In addition to the sequence, we pass two parameters: \n",
    "\n",
    "* `test_size` is the number that defines the size of the test set.\n",
    "* `random_state`, which is an integer that specifies that state of the random split.  **To make your tests reproducible, it is essential to set this parameter. Otherwise, you will get different splits each time you run your code.**\n",
    "\n",
    "`train_test_split()` performs the split and returns four sequences in this order:\n",
    "\n",
    "1. `X_train`: The training part of the first sequence (`X`)\n",
    "2. `X_test`: The test part of the first sequence (`X`)\n",
    "3. `y_train`: The training part of the second sequence (`y`)\n",
    "4. `y_test`: The test part of the second sequence (`y`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T12:02:48.404425Z",
     "iopub.status.busy": "2021-10-12T12:02:48.404139Z",
     "iopub.status.idle": "2021-10-12T12:02:48.411374Z",
     "shell.execute_reply": "2021-10-12T12:02:48.410475Z",
     "shell.execute_reply.started": "2021-10-12T12:02:48.404397Z"
    }
   },
   "outputs": [],
   "source": [
    "X = data.iloc[:,:-1] # features is all the columns except last one\n",
    "y = data.iloc[:,-1] # the last column is the label\n",
    "\n",
    "# split the data into training (70%) and testing (30%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.30, \n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model\n",
    "Now that data is split, it is time to fit the model. With scikit-learn, we do this in two lines of code. The first line creates a classifier object (i.e., `clf`) using the `DecisionTreeClassifier()` class we imported earlier. Here we pass two parameters to the `DecisionTreeClassifier()` constructor. The first (`max_depth`) specifies the maximum depth of the tree.  The second (`random_state`) is an integer that specifies the state of the random split. The decision tree algorithm randomly permutes the features for each split.  To ensure we get reproducible results across different runs, we must specify a `random_state`. \n",
    "\n",
    "The second line calls the `clf` object's `fit()` method by passing two parameters: `X_train` and `y_train`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T12:13:38.706482Z",
     "iopub.status.busy": "2021-10-12T12:13:38.70599Z",
     "iopub.status.idle": "2021-10-12T12:13:38.71652Z",
     "shell.execute_reply": "2021-10-12T12:13:38.715604Z",
     "shell.execute_reply.started": "2021-10-12T12:13:38.706446Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a classifer object and fit the model\n",
    "clf = DecisionTreeClassifier(max_depth = 2, random_state = 0)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Model\n",
    "\n",
    "To better understand the learned model, it is good to create a visualization of the decision tree. To produce a tree figure, we use the `plot_tree` function of the `tree` module. We pass to this function the classifier object (i.e., `clf`), the feature names list (`fn`), the class names list (`cn`), the axes to plot to (`axes`) and we set `filled` to `True` to paint nodes. See the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html) for more information on this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T12:20:05.14456Z",
     "iopub.status.busy": "2021-10-12T12:20:05.143557Z",
     "iopub.status.idle": "2021-10-12T12:20:05.646594Z",
     "shell.execute_reply": "2021-10-12T12:20:05.645852Z",
     "shell.execute_reply.started": "2021-10-12T12:20:05.144504Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot the decosion tree\n",
    "\n",
    "# feature names - required for filling the decision\n",
    "fn = ['variance','skewness','curtosis','entropy']\n",
    "\n",
    "# class names - required for filling the decicion tree\n",
    "cn = ['authentic', 'inauthentic']\n",
    "\n",
    "# create a figure object for plotting\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1,  figsize = (15,10))\n",
    "\n",
    "# plot the tree\n",
    "tree.plot_tree(clf,\n",
    "               feature_names=fn, \n",
    "               class_names=cn,\n",
    "               ax=axes,\n",
    "               filled=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to plotting the tree, we can get a textual representation of the decision tree using the `export_text` function. We pass two parameters to this functionâ€”the classifier object and feature names. \n",
    "\n",
    "See the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.tree.export_text.html?highlight=export_text#sklearn.tree.export_text) for the `export_text` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T12:26:12.546573Z",
     "iopub.status.busy": "2021-10-12T12:26:12.546224Z",
     "iopub.status.idle": "2021-10-12T12:26:12.553216Z",
     "shell.execute_reply": "2021-10-12T12:26:12.55229Z",
     "shell.execute_reply.started": "2021-10-12T12:26:12.546537Z"
    }
   },
   "outputs": [],
   "source": [
    "# export the decsion tree as a report\n",
    "r = export_text(clf, feature_names=fn)\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Last, we evaluate our model using various classification metrics by comparing our model's predictions against the ground truth.  We first call the `predict()` method by passing the `X_test` split of the data. The returned predictions for each of the examples in the `X_test` array is saved in `y_pred` array. We then pass the `y_pred` array along with the ground truth  (i.e., `y_test`) to the scoring functions  (e.g., `accuracy_score()` and  `r2_precision_score()`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-10-12T12:35:15.580254Z",
     "iopub.status.busy": "2021-10-12T12:35:15.579949Z",
     "iopub.status.idle": "2021-10-12T12:35:15.593938Z",
     "shell.execute_reply": "2021-10-12T12:35:15.593071Z",
     "shell.execute_reply.started": "2021-10-12T12:35:15.580223Z"
    }
   },
   "outputs": [],
   "source": [
    "# now we make some predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# let us evaluate the predictions\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy : {acc:.2f}\")\n",
    "\n",
    "pr = precision_score(y_test, y_pred)\n",
    "print(f\"Precision: {pr:.2f}\")\n",
    "\n",
    "re = recall_score(y_test, y_pred)\n",
    "print(f\"Recall   : {re:.2f}\")\n",
    "\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print(f\"F1 Score : {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see the model is $90\\%$ accurate; which means it is able to predict the corret class $90\\%$ of te time."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bf9a9eee405a0088de7ccf5832a78f5819a8f68f034f45858f658735d571460f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 64-bit ('Intro2AI': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
